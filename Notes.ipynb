{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976a8275",
   "metadata": {},
   "source": [
    "**Currently Learning From:**\n",
    "\n",
    "[LangChain_Official](https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x)\n",
    "\n",
    "[AI_Bites](https://www.youtube.com/playlist?list=PLcp6ZnH4WYlaYWCuDZ8oJNZaOFzmB8ciK)\n",
    "\n",
    "[FreeCodeCamp](https://www.youtube.com/watch?v=sVcwVQRHIc8)\n",
    "\n",
    "[Krish_Naik](https://www.youtube.com/playlist?list=PLZoTAELRMXVM8Pf4U67L4UuDRgV4TNX9D)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299367a",
   "metadata": {},
   "source": [
    "## **Introduction to RAG**\n",
    "\n",
    "Main reason for RAG: LLMs have a limited context window. RAG helps to overcome this limitation by retrieving relevant information from an external knowledge base.\n",
    "\n",
    "**Before RAG:**\n",
    "\n",
    "- Input Query → LLM → Output\n",
    "\n",
    "This approach relies solely on the knowledge encoded within the LLM, which may be outdated or insufficient for specific queries.\n",
    "\n",
    "This approach can lead to hallucinations, where the LLM generates plausible-sounding but incorrect or nonsensical answers.\n",
    "\n",
    "**With RAG:**\n",
    "\n",
    "- Input Query → Retriever → Relevant Documents + LLM → Output\n",
    "\n",
    "In this approach, the retriever fetches relevant documents from an external knowledge base based on the input query. The LLM then uses these documents to generate a more accurate and contextually relevant response.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Components of RAG:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a85d7",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4fd4e",
   "metadata": {},
   "source": [
    "## **What is `RetrievalQAChain`?**\n",
    "\n",
    "`RetrievalQAChain` is a specialized chain in the LangChain framework designed to facilitate question-answering tasks by leveraging a retrieval-based approach. It combines the capabilities of a language model with a document retrieval system to provide accurate and contextually relevant answers to user queries.\n",
    "\n",
    "### **Key Features:**\n",
    "\n",
    "- **Document Retrieval:** `RetrievalQAChain` integrates with various document retrieval systems (like vector stores or traditional databases) to fetch relevant documents based on the user's query.\n",
    "\n",
    "- **Contextual Understanding:** It uses a language model to understand the context of the retrieved documents and generate coherent answers.\n",
    "\n",
    "- **Customizable Prompts:** Users can customize the prompts used to query the language model, allowing for tailored responses based on specific requirements.\n",
    "\n",
    "- **Chain Integration:** It can be easily integrated with other chains in the LangChain framework, enabling complex workflows that involve multiple steps of processing.\n",
    "\n",
    "### **How It Works:**\n",
    "\n",
    "1. **Query Input:** The user provides a question or query.\n",
    "\n",
    "2. **Document Retrieval:** The chain uses the retrieval system to find documents that are relevant to the query.\n",
    "\n",
    "3. **Answer Generation:** The retrieved documents are then passed to the language model, which processes the information and generates an answer.\n",
    "\n",
    "4. **Output:** The final answer is returned to the user.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
