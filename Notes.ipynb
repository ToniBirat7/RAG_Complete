{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb77e38",
   "metadata": {},
   "source": [
    "**If Want to Use `Conda Env`**\n",
    "\n",
    "**Conda Activate**\n",
    "\n",
    "As our conda environment is stored in a external `HDD`, first we need to mount the `HDD` to access the conda environments.\n",
    "\n",
    "```bash\n",
    "# Mount the external HDD\n",
    "sudo mount -a\n",
    "```\n",
    "\n",
    "This will mount the external `HDD` where the conda environments are stored.\n",
    "\n",
    "```bash\n",
    "# Activate the base conda environment\n",
    "conda activate base\n",
    "```\n",
    "\n",
    "If not activated, then we need to run:\n",
    "\n",
    "```bash\n",
    "source ./.bashrc\n",
    "```\n",
    "\n",
    "Then we can activate the base conda environment using the above command.\n",
    "\n",
    "**NLP Conda Environment Activation**\n",
    "\n",
    "```bash\n",
    "# Run this command to activate the NLP conda environment\n",
    "conda activate nlp\n",
    "```\n",
    "\n",
    "After activating the base conda environment, you can activate the `nlp` conda environment using the above command. This environment contains all the necessary packages and dependencies for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a8275",
   "metadata": {},
   "source": [
    "**Currently Learning From:**\n",
    "\n",
    "[LangChain_Official](https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x)\n",
    "\n",
    "[AI_Bites](https://www.youtube.com/playlist?list=PLcp6ZnH4WYlaYWCuDZ8oJNZaOFzmB8ciK)\n",
    "\n",
    "[FreeCodeCamp](https://www.youtube.com/watch?v=sVcwVQRHIc8)\n",
    "\n",
    "[Krish_Naik](https://www.youtube.com/playlist?list=PLZoTAELRMXVM8Pf4U67L4UuDRgV4TNX9D)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa7557",
   "metadata": {},
   "source": [
    "## **Notes:**\n",
    "\n",
    "- Every `LLM` has a knowledge cutoff date. This means that the model does not have information about events or developments that occurred after that date. For example, GPT-3.5 has a knowledge cutoff date of September 2021, which means it does not have information about events or developments that occurred after that date.\n",
    "\n",
    "- Every `LLM` or `Embedding Model` has a context window limit. This means that the model can only process a certain amount of text at a time. For example, GPT-3.5 has a context window limit of 4096 tokens, which means it can only process up to 4096 tokens of text at a time.\n",
    "\n",
    "Therefore, we've to convert the `Query` or `Prompt` into `Chunks` such that each chunk is within the context window limit of the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299367a",
   "metadata": {},
   "source": [
    "## **Introduction to RAG**\n",
    "\n",
    "Main reason for RAG: LLMs have a limited context window. RAG helps to overcome this limitation by retrieving relevant information from an external knowledge base.\n",
    "\n",
    "**Before RAG:**\n",
    "\n",
    "- Input Query → LLM → Output\n",
    "\n",
    "This approach relies solely on the knowledge encoded within the LLM, which may be outdated or insufficient for specific queries.\n",
    "\n",
    "This approach can lead to hallucinations, where the LLM generates plausible-sounding but incorrect or nonsensical answers.\n",
    "\n",
    "**With RAG:**\n",
    "\n",
    "- Input Query → Retriever → Relevant Documents + LLM → Output\n",
    "\n",
    "In this approach, the retriever fetches relevant documents from an external knowledge base based on the input query. The LLM then uses these documents to generate a more accurate and contextually relevant response.\n",
    "\n",
    "This approach somehow mitigates the hallucination problem by grounding the LLM's responses in actual data retrieved from the knowledge base.\n",
    "\n",
    "This works, but **Can't we fine-tune the LLM directly on the external knowledge?**\n",
    "\n",
    "**Answer:** Fine-tuning LLMs on large external knowledge bases can be computationally expensive and time-consuming. RAG allows for dynamic retrieval of information without the need for extensive fine-tuning, making it more efficient and adaptable to changing knowledge.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **Components of RAG:**\n",
    "\n",
    "### **1. Retriever:**\n",
    "\n",
    "The retriever is responsible for fetching relevant documents or information from an external knowledge base based on the input query. The external knowledge is fetched from sources like Vector Database, Scraped Data, PDFs, etc.\n",
    "\n",
    "Types of Retrievers:\n",
    "\n",
    "- **Sparse Retrievers:** Use traditional methods like TF-IDF or BM25 to find relevant documents based on keyword matching.\n",
    "\n",
    "- **Dense Retrievers:** Use neural networks to create dense vector representations of documents and queries, allowing for more semantic matching. Examples include models like Sentence Transformers.\n",
    "\n",
    "### **2. Language Model (LLM):**\n",
    "\n",
    "The LLM generates responses based on the input query and the retrieved documents. It can be any large language model, such as GPT-3, GPT-4, or other open-source models.\n",
    "\n",
    "These things can be combined in different ways to create various RAG architectures, such as:\n",
    "\n",
    "- **Retrieve-then-Generate:** The retriever fetches relevant documents first, and then the LLM generates a response based on those documents.\n",
    "\n",
    "- **Generate-then-Retrieve:** The LLM generates an initial response, which is then refined using information from the retrieved documents.\n",
    "\n",
    "### **3. External Knowledge Base:**\n",
    "\n",
    "The external knowledge base is a repository of information from which the retriever fetches relevant documents. This can include databases, document collections, web pages, or any other structured or unstructured data source.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **Workflow of RAG:**\n",
    "\n",
    "1. **Input Query:** The user provides an input query that they want to get information about.\n",
    "\n",
    "2. **Parse Query:** The query is parsed and pre-processed to prepare it for retrieval.\n",
    "\n",
    "3. **Generate Embedding for Query:** The parsed query is converted into a vector representation (embedding) using a pre-trained model.\n",
    "\n",
    "4. **Retrieve Relevant Documents:** The retriever uses the query embedding to search the external knowledge base and fetch relevant documents.\n",
    "\n",
    "5. **Combine Query and Documents:** The input query and the retrieved documents are combined to form a context for the LLM.\n",
    "\n",
    "6. **Generate Response:** The LLM processes the combined context and generates a response to the input query.\n",
    "\n",
    "7. **Output Response:** The generated response is returned to the user.\n",
    "\n",
    "But, this is **Traditional RAG Workflow.**.\n",
    "\n",
    "There is **Agentic RAG Workflow** as well, where multiple agents can be used to handle different parts of the retrieval and generation process, allowing for more complex interactions and improved performance.\n",
    "\n",
    "<hr>\n",
    "\n",
    "One of the best example of `RAG` is **Perplexity AI**. It uses RAG to provide accurate and up-to-date answers by retrieving information from a vast knowledge base and generating responses using advanced LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fbf14d",
   "metadata": {},
   "source": [
    "## **Knowledge Base Creation**\n",
    "\n",
    "**What is a Knowledge Base?**\n",
    "\n",
    "A knowledge base is a structured repository of information that can be used to support the retrieval of relevant documents for RAG systems. It can consist of various types of data, such as text documents, PDFs, web pages, or any other form of unstructured or semi-structured data.\n",
    "\n",
    "It is basically a `Vector Database` that stores the vector representations of documents for efficient retrieval.\n",
    "\n",
    "There are several steps involved in creating a knowledge base for RAG systems.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Steps to Create a Knowledge Base**\n",
    "\n",
    "**1. Data Collection:**\n",
    "\n",
    "The first step is to gather the data that will form the basis of the knowledge base. This can involve scraping web pages, collecting documents, or using existing datasets.\n",
    "\n",
    "We need to collect all the relevant data sources. This includes documents, articles, PDFs, web pages, images, videos, audio files, etc.\n",
    "\n",
    "**2. Data Preprocessing:**\n",
    "\n",
    "Once the data is collected, it needs to be preprocessed to ensure consistency and quality. This may involve cleaning the text, removing duplicates, and standardizing formats.\n",
    "\n",
    "Preprocessing steps may include:\n",
    "\n",
    "- Text cleaning (removing special characters, HTML tags, etc.)\n",
    "\n",
    "- Tokenization (splitting text into words or sentences)\n",
    "\n",
    "- Normalization (lowercasing, stemming, lemmatization)\n",
    "\n",
    "- Removing stop words (common words that do not add much meaning)\n",
    "\n",
    "**3. Document Segmentation:**\n",
    "\n",
    "Large documents should be segmented into smaller, manageable chunks. This helps in better retrieval and ensures that the LLM can effectively utilize the information.\n",
    "\n",
    "Document segmentation can be done based on:\n",
    "\n",
    "- Paragraphs\n",
    "\n",
    "- Sentences\n",
    "\n",
    "- Fixed-size `Chunks` (e.g., 512 tokens)\n",
    "\n",
    "**4. Embedding Generation:**\n",
    "\n",
    "The next step is to convert the preprocessed text into vector representations (embeddings) using a suitable embedding model. These embeddings capture the semantic meaning of the text and allow for efficient similarity searches.\n",
    "\n",
    "Popular embedding models include:\n",
    "\n",
    "- Sentence Transformers (e.g., `all-MiniLM-L6-v2`, `paraphrase-MiniLM-L3-v2`)\n",
    "\n",
    "- OpenAI Embeddings (e.g., `text-embedding-ada-002`)\n",
    "\n",
    "- Image Embeddings (e.g., CLIP for images)\n",
    "\n",
    "- Multimodal Embeddings (e.g., models that combine text and image embeddings)\n",
    "\n",
    "**5. Saving the Embeddings:**\n",
    "\n",
    "Once the `embeddings` are generated, it is stored as a row in the below format in the vector database.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"constitution_2015_art35\",\n",
    "  \"vector\": [0.0123, -0.8841, ..., 0.0042],\n",
    "  \"payload\": {\n",
    "    \"text\": \"Article 35 – Right to Health...\",\n",
    "    \"article\": \"35\",\n",
    "    \"source\": \"Constitution of Nepal 2015\",\n",
    "    \"language\": \"ne\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "So, \n",
    "\n",
    "`vector`: Will be used for similarity search.\n",
    "\n",
    "`text`: Once the relevant vectors are retrieved, the corresponding text will be used as `context` for the LLM.\n",
    "\n",
    "`payload`: Additional metadata that can be useful for filtering or providing more context.\n",
    "\n",
    "So, we only store the `Semantic` meaning of the document in the form of `vector` and `text`. The rest of the information is optional.\n",
    "\n",
    "**6. Indexing:**\n",
    "\n",
    "Once the embeddings are generated, they need to be indexed in a vector database for efficient retrieval. Popular vector databases include:\n",
    "\n",
    "- `Milvus`\n",
    "\n",
    "- `Pinecone`\n",
    "\n",
    "- `Weaviate`\n",
    "\n",
    "- `PostgreSQL` with `pgvector` extension\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **Knowledge Graphs**\n",
    "\n",
    "A knowledge graph is a structured representation of knowledge that captures relationships between entities in a graph format. It consists of nodes (entities) and edges (relationships) that connect these entities.\n",
    "\n",
    "Knowledge graphs are used to represent complex relationships and enable reasoning over the data. They can be used in RAG systems to enhance the retrieval process by providing additional context and connections between entities.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Entities: \"Albert Einstein\", \"Theory of Relativity\", \"Physics\"\n",
    "\n",
    "- Relationships: \"Albert Einstein\" → \"developed\" → \"Theory of Relativity\"\n",
    "\n",
    "Knowledge graphs can be constructed using various techniques, including:\n",
    "\n",
    "- Manual curation\n",
    "\n",
    "- Automated extraction from text using natural language processing (NLP) techniques\n",
    "\n",
    "- Integration of existing structured data sources (e.g., databases, ontologies)\n",
    "\n",
    "Knowledge graphs can be stored in graph databases such as `Neo4j`, `Amazon Neptune`, or RDF stores.\n",
    "\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6b4010",
   "metadata": {},
   "source": [
    "**RAG Pipeline Architecture**\n",
    "\n",
    "To build a production-ready RAG pipeline, there are two main sub pipelines:\n",
    "\n",
    "## **Data Ingestion Pipeline**\n",
    "\n",
    "The data ingestion pipeline is responsible for collecting and preprocessing data from various sources to create a knowledge base.\n",
    "\n",
    "Here, perform:\n",
    "\n",
    "### **Data Parsing**\n",
    "\n",
    "Extract text and metadata from various data sources such as documents, web pages, PDFs, etc.\n",
    "\n",
    "**Convert into Document Structure:** For any kind of data be it text, image, audio, video. All of these data have `Metadata`, `Content` and Other attributes. We just can't keep only the content. We need to keep the metadata as well for better retrieval. So, convert all the data into a common document structure that includes content and metadata.\n",
    "\n",
    "Why do we need `Metadata`?\n",
    "\n",
    "- Metadata provides additional context about the content, such as the source, author, date of creation, and other relevant attributes. This information can be crucial for improving the accuracy and relevance of document retrieval during the RAG process.\n",
    "\n",
    "- We can use metadata to filter and rank documents during retrieval, ensuring that the most relevant and trustworthy sources are prioritized.\n",
    "\n",
    "### **Chunking**\n",
    "\n",
    "Break down large documents into smaller chunks to ensure that they fit within the context window of the LLM and improve retrieval efficiency.\n",
    "\n",
    "While chunking, there are various strategies like:\n",
    "\n",
    "- Overlapping Chunks: Create chunks with overlapping content to preserve context.\n",
    "\n",
    "- Fixed-size Chunks: Create chunks of a fixed size (e.g., 512 tokens) for consistency.\n",
    "\n",
    "Once the data is parsed and chunked,\n",
    "\n",
    "It is fed into the embedding model to generate embeddings for each chunk.\n",
    "\n",
    "The generated embeddings are then stored in a vector database to create the knowledge base.\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "## **Retrieval Pipeline**\n",
    "\n",
    "The retrieval pipeline is responsible for fetching relevant documents from the external knowledge base based on user queries. This may involve:\n",
    "\n",
    "- Receiving the user query and generating an embedding for it.\n",
    "\n",
    "- Searching the external knowledge base for relevant documents using the query embedding.\n",
    "\n",
    "- Ranking the retrieved documents based on their relevance to the query.\n",
    "\n",
    "- Returning the top-ranked documents to the language model for response generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a85d7",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4fd4e",
   "metadata": {},
   "source": [
    "## **What is `RetrievalQAChain`?**\n",
    "\n",
    "`RetrievalQAChain` is a specialized chain in the LangChain framework designed to facilitate question-answering tasks by leveraging a retrieval-based approach. It combines the capabilities of a language model with a document retrieval system to provide accurate and contextually relevant answers to user queries.\n",
    "\n",
    "### **Key Features:**\n",
    "\n",
    "- **Document Retrieval:** `RetrievalQAChain` integrates with various document retrieval systems (like vector stores or traditional databases) to fetch relevant documents based on the user's query.\n",
    "\n",
    "- **Contextual Understanding:** It uses a language model to understand the context of the retrieved documents and generate coherent answers.\n",
    "\n",
    "- **Customizable Prompts:** Users can customize the prompts used to query the language model, allowing for tailored responses based on specific requirements.\n",
    "\n",
    "- **Chain Integration:** It can be easily integrated with other chains in the LangChain framework, enabling complex workflows that involve multiple steps of processing.\n",
    "\n",
    "### **How It Works:**\n",
    "\n",
    "1. **Query Input:** The user provides a question or query.\n",
    "\n",
    "2. **Document Retrieval:** The chain uses the retrieval system to find documents that are relevant to the query.\n",
    "\n",
    "3. **Answer Generation:** The retrieved documents are then passed to the language model, which processes the information and generates an answer.\n",
    "\n",
    "4. **Output:** The final answer is returned to the user.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
