{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976a8275",
   "metadata": {},
   "source": [
    "**Currently Learning From:**\n",
    "\n",
    "[LangChain_Official](https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x)\n",
    "\n",
    "[AI_Bites](https://www.youtube.com/playlist?list=PLcp6ZnH4WYlaYWCuDZ8oJNZaOFzmB8ciK)\n",
    "\n",
    "[FreeCodeCamp](https://www.youtube.com/watch?v=sVcwVQRHIc8)\n",
    "\n",
    "[Krish_Naik](https://www.youtube.com/playlist?list=PLZoTAELRMXVM8Pf4U67L4UuDRgV4TNX9D)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299367a",
   "metadata": {},
   "source": [
    "## **Introduction to RAG**\n",
    "\n",
    "Main reason for RAG: LLMs have a limited context window. RAG helps to overcome this limitation by retrieving relevant information from an external knowledge base.\n",
    "\n",
    "**Before RAG:**\n",
    "\n",
    "- Input Query → LLM → Output\n",
    "\n",
    "This approach relies solely on the knowledge encoded within the LLM, which may be outdated or insufficient for specific queries.\n",
    "\n",
    "This approach can lead to hallucinations, where the LLM generates plausible-sounding but incorrect or nonsensical answers.\n",
    "\n",
    "**With RAG:**\n",
    "\n",
    "- Input Query → Retriever → Relevant Documents + LLM → Output\n",
    "\n",
    "In this approach, the retriever fetches relevant documents from an external knowledge base based on the input query. The LLM then uses these documents to generate a more accurate and contextually relevant response.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **Components of RAG:**\n",
    "\n",
    "### **1. Retriever:**\n",
    "\n",
    "The retriever is responsible for fetching relevant documents or information from an external knowledge base based on the input query. The external knowledge is fetched from sources like Vector Database, Scraped Data, PDFs, etc.\n",
    "\n",
    "Types of Retrievers:\n",
    "\n",
    "- **Sparse Retrievers:** Use traditional methods like TF-IDF or BM25 to find relevant documents based on keyword matching.\n",
    "\n",
    "- **Dense Retrievers:** Use neural networks to create dense vector representations of documents and queries, allowing for more semantic matching. Examples include models like Sentence Transformers.\n",
    "\n",
    "### **2. Language Model (LLM):**\n",
    "\n",
    "The LLM generates responses based on the input query and the retrieved documents. It can be any large language model, such as GPT-3, GPT-4, or other open-source models.\n",
    "\n",
    "These things can be combined in different ways to create various RAG architectures, such as:\n",
    "\n",
    "- **Retrieve-then-Generate:** The retriever fetches relevant documents first, and then the LLM generates a response based on those documents.\n",
    "\n",
    "- **Generate-then-Retrieve:** The LLM generates an initial response, which is then refined using information from the retrieved documents.\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a85d7",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4fd4e",
   "metadata": {},
   "source": [
    "## **What is `RetrievalQAChain`?**\n",
    "\n",
    "`RetrievalQAChain` is a specialized chain in the LangChain framework designed to facilitate question-answering tasks by leveraging a retrieval-based approach. It combines the capabilities of a language model with a document retrieval system to provide accurate and contextually relevant answers to user queries.\n",
    "\n",
    "### **Key Features:**\n",
    "\n",
    "- **Document Retrieval:** `RetrievalQAChain` integrates with various document retrieval systems (like vector stores or traditional databases) to fetch relevant documents based on the user's query.\n",
    "\n",
    "- **Contextual Understanding:** It uses a language model to understand the context of the retrieved documents and generate coherent answers.\n",
    "\n",
    "- **Customizable Prompts:** Users can customize the prompts used to query the language model, allowing for tailored responses based on specific requirements.\n",
    "\n",
    "- **Chain Integration:** It can be easily integrated with other chains in the LangChain framework, enabling complex workflows that involve multiple steps of processing.\n",
    "\n",
    "### **How It Works:**\n",
    "\n",
    "1. **Query Input:** The user provides a question or query.\n",
    "\n",
    "2. **Document Retrieval:** The chain uses the retrieval system to find documents that are relevant to the query.\n",
    "\n",
    "3. **Answer Generation:** The retrieved documents are then passed to the language model, which processes the information and generates an answer.\n",
    "\n",
    "4. **Output:** The final answer is returned to the user.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
