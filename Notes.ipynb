{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976a8275",
   "metadata": {},
   "source": [
    "**Currently Learning From:**\n",
    "\n",
    "[LangChain_Official](https://www.youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x)\n",
    "\n",
    "[AI_Bites](https://www.youtube.com/playlist?list=PLcp6ZnH4WYlaYWCuDZ8oJNZaOFzmB8ciK)\n",
    "\n",
    "[FreeCodeCamp](https://www.youtube.com/watch?v=sVcwVQRHIc8)\n",
    "\n",
    "[Krish_Naik](https://www.youtube.com/playlist?list=PLZoTAELRMXVM8Pf4U67L4UuDRgV4TNX9D)\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299367a",
   "metadata": {},
   "source": [
    "## **Introduction to RAG**\n",
    "\n",
    "Main reason for RAG: LLMs have a limited context window. RAG helps to overcome this limitation by retrieving relevant information from an external knowledge base.\n",
    "\n",
    "**Before RAG:**\n",
    "\n",
    "- Input Query → LLM → Output\n",
    "\n",
    "This approach relies solely on the knowledge encoded within the LLM, which may be outdated or insufficient for specific queries.\n",
    "\n",
    "This approach can lead to hallucinations, where the LLM generates plausible-sounding but incorrect or nonsensical answers.\n",
    "\n",
    "**With RAG:**\n",
    "\n",
    "- Input Query → Retriever → Relevant Documents + LLM → Output\n",
    "\n",
    "In this approach, the retriever fetches relevant documents from an external knowledge base based on the input query. The LLM then uses these documents to generate a more accurate and contextually relevant response.\n",
    "\n",
    "This approach somehow mitigates the hallucination problem by grounding the LLM's responses in actual data retrieved from the knowledge base.\n",
    "\n",
    "This works, but **Can't we fine-tune the LLM directly on the external knowledge?**\n",
    "\n",
    "**Answer:** Fine-tuning LLMs on large external knowledge bases can be computationally expensive and time-consuming. RAG allows for dynamic retrieval of information without the need for extensive fine-tuning, making it more efficient and adaptable to changing knowledge.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **Components of RAG:**\n",
    "\n",
    "### **1. Retriever:**\n",
    "\n",
    "The retriever is responsible for fetching relevant documents or information from an external knowledge base based on the input query. The external knowledge is fetched from sources like Vector Database, Scraped Data, PDFs, etc.\n",
    "\n",
    "Types of Retrievers:\n",
    "\n",
    "- **Sparse Retrievers:** Use traditional methods like TF-IDF or BM25 to find relevant documents based on keyword matching.\n",
    "\n",
    "- **Dense Retrievers:** Use neural networks to create dense vector representations of documents and queries, allowing for more semantic matching. Examples include models like Sentence Transformers.\n",
    "\n",
    "### **2. Language Model (LLM):**\n",
    "\n",
    "The LLM generates responses based on the input query and the retrieved documents. It can be any large language model, such as GPT-3, GPT-4, or other open-source models.\n",
    "\n",
    "These things can be combined in different ways to create various RAG architectures, such as:\n",
    "\n",
    "- **Retrieve-then-Generate:** The retriever fetches relevant documents first, and then the LLM generates a response based on those documents.\n",
    "\n",
    "- **Generate-then-Retrieve:** The LLM generates an initial response, which is then refined using information from the retrieved documents.\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **Workflow of RAG:**\n",
    "\n",
    "1. **Input Query:** The user provides an input query that they want to get information about.\n",
    "\n",
    "2. **Parse Query:** The query is parsed and pre-processed to prepare it for retrieval.\n",
    "\n",
    "3. **Generate Embedding for Query:** The parsed query is converted into a vector representation (embedding) using a pre-trained model.\n",
    "\n",
    "4. **Retrieve Relevant Documents:** The retriever uses the query embedding to search the external knowledge base and fetch relevant documents.\n",
    "\n",
    "5. **Combine Query and Documents:** The input query and the retrieved documents are combined to form a context for the LLM.\n",
    "\n",
    "6. **Generate Response:** The LLM processes the combined context and generates a response to the input query.\n",
    "\n",
    "7. **Output Response:** The generated response is returned to the user.\n",
    "\n",
    "But, this is **Traditional RAG Workflow.**.\n",
    "\n",
    "There is **Agentic RAG Workflow** as well, where multiple agents can be used to handle different parts of the retrieval and generation process, allowing for more complex interactions and improved performance.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fbf14d",
   "metadata": {},
   "source": [
    "## **Knowledge Base Creation**\n",
    "\n",
    "**What is a Knowledge Base?**\n",
    "\n",
    "A knowledge base is a structured repository of information that can be used to support the retrieval of relevant documents for RAG systems. It can consist of various types of data, such as text documents, PDFs, web pages, or any other form of unstructured or semi-structured data.\n",
    "\n",
    "It is basically a `Vector Database` that stores the vector representations of documents for efficient retrieval.\n",
    "\n",
    "There are several steps involved in creating a knowledge base for RAG systems.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Steps to Create a Knowledge Base**\n",
    "\n",
    "**1. Data Collection:**\n",
    "\n",
    "The first step is to gather the data that will form the basis of the knowledge base. This can involve scraping web pages, collecting documents, or using existing datasets.\n",
    "\n",
    "We need to collect all the relevant data sources. This includes documents, articles, PDFs, web pages, images, videos, audio files, etc.\n",
    "\n",
    "**2. Data Preprocessing:**\n",
    "\n",
    "Once the data is collected, it needs to be preprocessed to ensure consistency and quality. This may involve cleaning the text, removing duplicates, and standardizing formats.\n",
    "\n",
    "Preprocessing steps may include:\n",
    "\n",
    "- Text cleaning (removing special characters, HTML tags, etc.)\n",
    "\n",
    "- Tokenization (splitting text into words or sentences)\n",
    "\n",
    "- Normalization (lowercasing, stemming, lemmatization)\n",
    "\n",
    "- Removing stop words (common words that do not add much meaning)\n",
    "\n",
    "**3. Document Segmentation:**\n",
    "\n",
    "Large documents should be segmented into smaller, manageable chunks. This helps in better retrieval and ensures that the LLM can effectively utilize the information.\n",
    "\n",
    "Document segmentation can be done based on:\n",
    "\n",
    "- Paragraphs\n",
    "\n",
    "- Sentences\n",
    "\n",
    "- Fixed-size `Chunks` (e.g., 512 tokens)\n",
    "\n",
    "**4. Embedding Generation:**\n",
    "\n",
    "The next step is to convert the preprocessed text into vector representations (embeddings) using a suitable embedding model. These embeddings capture the semantic meaning of the text and allow for efficient similarity searches.\n",
    "\n",
    "Popular embedding models include:\n",
    "\n",
    "- Sentence Transformers (e.g., `all-MiniLM-L6-v2`, `paraphrase-MiniLM-L3-v2`)\n",
    "\n",
    "- OpenAI Embeddings (e.g., `text-embedding-ada-002`)\n",
    "\n",
    "- Image Embeddings (e.g., CLIP for images)\n",
    "\n",
    "- Multimodal Embeddings (e.g., models that combine text and image embeddings)\n",
    "\n",
    "**5. Indexing:**\n",
    "\n",
    "Once the embeddings are generated, they need to be indexed in a vector database for efficient retrieval. Popular vector databases include:\n",
    "\n",
    "- Milvus\n",
    "\n",
    "- Pinecone\n",
    "\n",
    "- Weaviate\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a85d7",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4fd4e",
   "metadata": {},
   "source": [
    "## **What is `RetrievalQAChain`?**\n",
    "\n",
    "`RetrievalQAChain` is a specialized chain in the LangChain framework designed to facilitate question-answering tasks by leveraging a retrieval-based approach. It combines the capabilities of a language model with a document retrieval system to provide accurate and contextually relevant answers to user queries.\n",
    "\n",
    "### **Key Features:**\n",
    "\n",
    "- **Document Retrieval:** `RetrievalQAChain` integrates with various document retrieval systems (like vector stores or traditional databases) to fetch relevant documents based on the user's query.\n",
    "\n",
    "- **Contextual Understanding:** It uses a language model to understand the context of the retrieved documents and generate coherent answers.\n",
    "\n",
    "- **Customizable Prompts:** Users can customize the prompts used to query the language model, allowing for tailored responses based on specific requirements.\n",
    "\n",
    "- **Chain Integration:** It can be easily integrated with other chains in the LangChain framework, enabling complex workflows that involve multiple steps of processing.\n",
    "\n",
    "### **How It Works:**\n",
    "\n",
    "1. **Query Input:** The user provides a question or query.\n",
    "\n",
    "2. **Document Retrieval:** The chain uses the retrieval system to find documents that are relevant to the query.\n",
    "\n",
    "3. **Answer Generation:** The retrieved documents are then passed to the language model, which processes the information and generates an answer.\n",
    "\n",
    "4. **Output:** The final answer is returned to the user.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
